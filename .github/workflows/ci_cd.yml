# File: .github/workflows/ci_cd.yml
name: ML Model CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

env:
  # Define the required accuracy threshold (your Quality Gate)
  MIN_ACCURACY_THRESHOLD: 0.90
  
jobs:
  # --- 1. Continuous Integration (Validation) ---
  build_and_test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Basic Unit Tests (Validation)
        run: |
          echo "Running simple CI checks..."
          python -c "assert 1 == 1"
          echo "CI checks passed."
      
  # ----------------------------------------------------------------------

  # --- 2. Continuous Delivery (Training and Artifact Management) ---
  train_and_deploy:
    needs: build_and_test # Only run if CI passes
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # --- Training Step: Email Ingestion, Training, and Metric Extraction ---
      - name: Build and Run Training Container
        id: train
        run: |
          IMAGE_NAME="ml-agent-trainer"
          docker build -t $IMAGE_NAME .
          
          echo "Starting Docker container with email credentials..."
          
          # Run the container and capture all output
          # CRITICAL FIX: Ensure secrets are passed as -e arguments to the 'docker run' command
          TRAINING_OUTPUT=$(docker run \
            -e AGENT_EMAIL="${{ secrets.AGENT_EMAIL }}" \
            -e AGENT_PASSWORD="${{ secrets.AGENT_PASSWORD }}" \
            $IMAGE_NAME)
          
          echo "--- TRAINING LOG START ---"
          echo "$TRAINING_OUTPUT"
          echo "--- TRAINING LOG END ---"
          
          # CRITICAL: Parse the accuracy from the output string (***METRIC_OUTPUT***)
          # We use grep and awk to reliably extract the float number.
          ACCURACY=$(echo "$TRAINING_OUTPUT" | grep '***METRIC_OUTPUT***' | awk -F'=' '{print $2}' | awk -F'.' '{print $1"."substr($2,1,4)}')
          
          echo "Extracted Model Accuracy: $ACCURACY"
          
          # Set the extracted accuracy as an output variable for the next steps
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT

      # --- Quality Gate: Check Accuracy ---
      - name: Check Model Performance against Threshold
        id: check_gate
        run: |
          # Use 'bc' (basic calculator) for float comparison
          # This step FAILS the job if the accuracy is too low.
          CURRENT_ACCURACY="${{ steps.train.outputs.accuracy }}"
          MIN_THRESHOLD="${{ env.MIN_ACCURACY_THRESHOLD }}"

          if [ -z "$CURRENT_ACCURACY" ] || echo "$CURRENT_ACCURACY < $MIN_THRESHOLD" | bc -l; then
            echo "Model failed Quality Gate. Accuracy $CURRENT_ACCURACY is below $MIN_THRESHOLD."
            exit 1 # Fail the job
          else
            echo "Model passed Quality Gate. Accuracy $CURRENT_ACCURACY is OK."
          fi

      # --- Artifact Deployment ---
      - name: Upload Model and Metrics Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifact-${{ steps.train.outputs.accuracy }}
          path: |
            model.pkl
            metrics.txt
          retention-days: 7
