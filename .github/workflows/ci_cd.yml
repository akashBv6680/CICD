# File: .github/workflows/ci_cd.yml
name: ML Model CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

env:
  MIN_ACCURACY_THRESHOLD: 0.90
  
jobs:
  # ... (build_and_test job remains the same)

  # --- 2. Continuous Delivery (Training and Artifact Management) ---
  train_and_deploy:
    needs: build_and_test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Run Training Container (Final Debugging Execution)
        id: train
        # The entire execution logic is within this step's run block
        run: |
          IMAGE_NAME="ml-agent-trainer"
          docker build -t $IMAGE_NAME .
          
          echo "Starting Docker container with email credentials..."
          
          # CRITICAL FIX: Run the Python script directly.
          # We use a subshell to execute Python and explicitly check its exit code ($?)
          # The entire output (stdout and stderr) is captured in a variable.
          
          # This command attempts the run, captures output, and uses '|| true' to prevent shell failure.
          TRAINING_OUTPUT=$(docker run \
            -e AGENT_EMAIL="${{ secrets.AGENT_EMAIL }}" \
            -e AGENT_PASSWORD="${{ secrets.AGENT_PASSWORD }}" \
            $IMAGE_NAME python train.py 2>&1) || true
          
          # Check the exit code of the actual docker run process (not the subshell capture)
          # Note: $? holds the exit code of the last foreground command (the docker run)
          DOCKER_EXIT_CODE=$?
          
          echo "--- TRAINING LOG START (Includes Errors) ---"
          echo "$TRAINING_OUTPUT"
          echo "--- TRAINING LOG END ---"
          echo "Docker Container Exit Code was: $DOCKER_EXIT_CODE"

          # --- CHECK FOR CRITICAL FAILURES ---
          # 1. Check for the custom FATAL error message (from train.py)
          if echo "$TRAINING_OUTPUT" | grep -q "FATAL WORKFLOW ERROR"; then
              echo "::error::Critical failure detected in train.py. Check logs for the FULL TRACEBACK above."
              exit 1 
          fi
          
          # 2. Check the raw exit code (if the crash happened before FATAL message was printed)
          if [ "$DOCKER_EXIT_CODE" -ne 0 ] && ! echo "$TRAINING_OUTPUT" | grep -q '***METRIC_OUTPUT***'; then
              echo "::error::Container exited with code $DOCKER_EXIT_CODE and no metrics were found."
              echo "The script likely crashed during IMAP login or environment setup."
              exit 1
          fi

          # --- PARSE METRICS IF NO FATAL ERROR DETECTED ---
          ACCURACY=$(echo "$TRAINING_OUTPUT" | grep '***METRIC_OUTPUT***' | awk -F'=' '{print $2}' | awk -F'.' '{print $1"."substr($2,1,4)}')
          
          if [ -z "$ACCURACY" ]; then
              echo "::error::Could not extract accuracy score. Training failed."
              exit 1 
          fi
          
          echo "Extracted Model Accuracy: $ACCURACY"
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT

      # ... (Quality Gate and Upload Artifacts steps remain the same)
