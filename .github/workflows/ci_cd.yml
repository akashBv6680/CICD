# File: .github/workflows/ci_cd.yml
name: ML Model CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

env:
  # Define the required accuracy threshold (your Quality Gate)
  MIN_ACCURACY_THRESHOLD: 0.90
  
jobs:
  # --- 1. Continuous Integration (Validation) ---
  build_and_test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Basic Unit Tests (Validation)
        run: |
          echo "CI checks passed."
      
  # ----------------------------------------------------------------------

  # --- 2. Continuous Delivery (Training and Artifact Management) ---
  train_and_deploy:
    needs: build_and_test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # --- Training Step: Email Ingestion, Training, and Metric Extraction ---
      - name: Build and Run Training Container (Robust Error Capture)
        id: train
        run: |
          IMAGE_NAME="ml-agent-trainer"
          docker build -t $IMAGE_NAME .
          
          echo "Starting Docker container with email credentials..."
          
          # CRITICAL: Run the Python script directly, redirecting stderr (2) to stdout (1).
          # This captures the full Python traceback in the TRAINING_OUTPUT variable.
          TRAINING_OUTPUT=$(docker run \
            -e AGENT_EMAIL="${{ secrets.AGENT_EMAIL }}" \
            -e AGENT_PASSWORD="${{ secrets.AGENT_PASSWORD }}" \
            $IMAGE_NAME /usr/bin/python3 train.py 2>&1)
          
          echo "--- TRAINING LOG START (Includes Errors) ---"
          echo "$TRAINING_OUTPUT"
          echo "--- TRAINING LOG END ---"
          
          # --- CHECK FOR CRITICAL FAILURES ---
          # Look for the error message intentionally printed by train.py
          if echo "$TRAINING_OUTPUT" | grep -q "FATAL WORKFLOW ERROR"; then
              echo "::error::Critical failure detected in train.py. Check logs for the FULL TRACEBACK above."
              # Force failure if the script reported a FATAL error
              exit 1 
          fi

          # --- PARSE METRICS IF NO FATAL ERROR DETECTED ---
          # Extract the accuracy value
          ACCURACY=$(echo "$TRAINING_OUTPUT" | grep '***METRIC_OUTPUT***' | awk -F'=' '{print $2}' | awk -F'.' '{print $1"."substr($2,1,4)}')
          
          if [ -z "$ACCURACY" ]; then
              echo "::error::Could not extract accuracy score. Model training likely failed silently."
              exit 1 
          fi
          
          echo "Extracted Model Accuracy: $ACCURACY"
          
          # Set the extracted accuracy as an output variable for the next steps
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT

      # --- Quality Gate: Check Accuracy ---
      - name: Check Model Performance against Threshold
        id: check_gate
        run: |
          # Use 'bc' (basic calculator) for float comparison
          CURRENT_ACCURACY="${{ steps.train.outputs.accuracy }}"
          MIN_THRESHOLD="${{ env.MIN_ACCURACY_THRESHOLD }}"

          if [ -z "$CURRENT_ACCURACY" ] || echo "$CURRENT_ACCURACY < $MIN_THRESHOLD" | bc -l; then
            echo "Model failed Quality Gate. Accuracy $CURRENT_ACCURACY is below $MIN_THRESHOLD."
            exit 1 
          else
            echo "Model passed Quality Gate. Accuracy $CURRENT_ACCURACY is OK."
          fi

      # --- Artifact Deployment ---
      - name: Upload Model and Metrics Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifact-${{ steps.train.outputs.accuracy }}
          path: |
            model.pkl
            metrics.txt
          retention-days: 7
