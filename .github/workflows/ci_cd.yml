# File: .github/workflows/ci_cd.yml
name: ML Model CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

env:
  MIN_ACCURACY_THRESHOLD: 0.70 
  
jobs:
  # --- 1. Continuous Integration (Validation) ---
  build_and_test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Basic Unit Tests (Validation)
        run: echo "CI checks passed."
      
  # ----------------------------------------------------------------------

  # --- 2. Continuous Delivery (Training and Artifact Management) ---
  train_and_deploy:
    needs: build_and_test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      # --- Build and Run Training Container (FIXED: Added Volume Mount) ---
      - name: Build and Run Training Container
        id: train
        run: |
          IMAGE_NAME="ml-agent-trainer"
          
          # 1. Create a directory on the host runner to store the artifacts
          mkdir -p artifacts
          
          docker build -t $IMAGE_NAME .
          
          echo "Starting Docker container with email credentials..."
          
          # 2. Run container, mapping the host 'artifacts' folder to '/app/output' inside the container
          #    Files saved by train.py to /app/output are now accessible on the host via ./artifacts
          docker run \
            -e AGENT_EMAIL="${{ secrets.AGENT_EMAIL }}" \
            -e AGENT_PASSWORD="${{ secrets.AGENT_PASSWORD }}" \
            -v $(pwd)/artifacts:/app/output \
            $IMAGE_NAME python train.py > train_output.log 2>&1 || true

          DOCKER_EXIT_CODE=$?
          TRAINING_OUTPUT=$(cat train_output.log)
          
          echo "--- TRAINING LOG START (Includes Errors) ---"
          echo "$TRAINING_OUTPUT"
          echo "--- TRAINING LOG END ---"
          echo "Docker Container Exit Code was: $DOCKER_EXIT_CODE"

          # --- CHECK FOR CRITICAL FAILURES ---
          if echo "$TRAINING_OUTPUT" | grep -q "FATAL WORKFLOW ERROR"; then
              
              if echo "$TRAINING_OUTPUT" | grep -q "NO_NEW_EMAILS_FOUND"; then
                  echo "::warning::Training skipped. Reason: No new emails found. Exiting gracefully."
                  echo "accuracy=0.0000" >> $GITHUB_OUTPUT 
                  exit 0
              fi
              
              echo "::error::Critical failure detected in train.py. Check logs for the FULL TRACEBACK above."
              exit 1 
          fi
          
          if [ "$DOCKER_EXIT_CODE" -ne 0 ]; then
              echo "::error::Container exited with unhandled code $DOCKER_EXIT_CODE. Script crashed unexpectedly."
              exit 1
          fi

          # --- PARSE METRICS IF SUCCESSFUL ---
          METRIC_LINE=$(echo "$TRAINING_OUTPUT" | grep '***METRIC_OUTPUT***')
          
          if [ -z "$METRIC_LINE" ]; then
              echo "::error::Could not extract metric. Script ran, but metric line was not found."
              exit 1 
          fi
          
          METRIC_VALUE=$(echo "$METRIC_LINE" | awk -F'=' '{print $2}' | awk -F'[*]' '{print $1}' | awk '{printf "%.4f", $1}')
          
          echo "Extracted Model Metric Value: $METRIC_VALUE"
          echo "accuracy=$METRIC_VALUE" >> $GITHUB_OUTPUT

      # --- Quality Gate: Check Performance (Using reliable AWK comparison) ---
      - name: Check Model Performance against Threshold
        id: check_gate
        run: |
          CURRENT_ACCURACY="${{ steps.train.outputs.accuracy }}"
          MIN_THRESHOLD="${{ env.MIN_ACCURACY_THRESHOLD }}"

          if [ -z "$CURRENT_ACCURACY" ]; then
             echo "::error::Quality Gate Failed: Accuracy value is missing entirely (Workflow Logic Error)."
             exit 1
          fi

          if [ "$CURRENT_ACCURACY" == "0.0000" ]; then
             echo "::error::Quality Gate Failed: Training was skipped because 'No New Emails Found'."
             exit 1
          fi

          COMPARE_RESULT=$(echo | awk -v current="$CURRENT_ACCURACY" -v min="$MIN_THRESHOLD" 'BEGIN { 
            if (current < min) {
              print 1
            } else {
              print 0
            }
          }')

          if [ "$COMPARE_RESULT" -eq 1 ]; then
            echo "::error::Quality Gate Failed: Performance $CURRENT_ACCURACY is below $MIN_THRESHOLD."
            exit 1 
          else
            echo "Model passed Quality Gate. Performance $CURRENT_ACCURACY is OK."
          fi

      # --- Artifact Deployment (FIXED: Path is now the host directory 'artifacts/') ---
      - name: Upload Model and Metrics Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifact-${{ steps.train.outputs.accuracy }}
          path: artifacts/ # <--- The upload step now looks here on the host runner
          retention-days: 7
